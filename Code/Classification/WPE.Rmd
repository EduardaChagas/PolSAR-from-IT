---
title: "Classification of Regions with WPE"
author: "Eduarda Chagas"
date: "May 2, 2020"
output:
  pdf_document: default
  html_notebook: default
---

In this script, we will evaluate the performance of the WPE technique for region classification in PolSAR textures.

###Importing the packages

```{r}
# Load some packages: 
if(!require(caret)) install.packages("caret")
if(!require(MLmetrics)) install.packages("MLmetrics")

setwd("/home/eduarda/Desktop/Repositories/SAR/SAR-WATG-master/Code/Classification")
```

###Importing the dataset

For this analysis, three SAR images with different regions were used, they are:

* Sierra del Lacandon National Park, Guatemala (purchased April 10, 2015), available at [https://uavsar.jpl.nasa.gov/cgi-bin/product.pl?jobName=Lacand_30202_15043_
006_150410_L090_CX_01 # data] (https://uavsar.jpl.nasa.gov/cgi-bin/product.pl?jobName=Lacand_30202_15043_
006_150410_L090_CX_01 # data);

* Oceanic regions of Cape Canaveral (acquired on September 22, 2016);

* Urban area of the city of Munich, Germany (acquired on June 5, 2015).

A total of 160 samples were considered during the investigation, with 40 forest regions in Guatemala, 80 ocean regions in Cape Canaveral and 40 urban regions in the city of Munich.

```{r}
n.total = 200
regions = c(rep("Forest",40), rep("Sea",80), rep("Urban", 40), rep("Pasture", 40))

Entropy.Complexity = data.frame("Entropy" = numeric(n.total), 
                                "Complexity" = numeric(n.total),
                                "Region" = character(n.total), 
                                stringsAsFactors=FALSE)

Entropy.Complexity.csv = read.csv(file="../../Data/EntropyComplexityWPED3T1.csv", 
                                  header=TRUE, sep=",")
Entropy.Complexity$Entropy = Entropy.Complexity.csv[,1]
Entropy.Complexity$Complexity = Entropy.Complexity.csv[,2]
Entropy.Complexity$Region = regions


split = 0.85
trainIndex = createDataPartition(Entropy.Complexity$Region, p = split, list = FALSE)

x = data.frame(Entropy.Complexity$Entropy[trainIndex], Entropy.Complexity$Complexity[trainIndex])
y = factor(Entropy.Complexity$Region[trainIndex])

x_validation = data.frame("Entropy" = Entropy.Complexity$Entropy[-trainIndex], "Complexity" = Entropy.Complexity$Complexity[-trainIndex])
y_validation = factor(Entropy.Complexity$Region[-trainIndex])

Entropy.Complexity = data.frame("Entropy" = Entropy.Complexity$Entropy[trainIndex], 
                                "Complexity" = Entropy.Complexity$Complexity[trainIndex],
                                "Region" = Entropy.Complexity$Region[trainIndex], 
                                stringsAsFactors=FALSE)
```

##KNN Classifier

###Creating KNN model and predicting
```{r}
set.seed(123)
ctrl = trainControl(method="repeatedcv", number = 10, repeats = 10)
knnFit = train(Region~., data = Entropy.Complexity, method = "knn", 
               trControl = ctrl, 
               preProcess = c("center","scale"), 
               tuneLength = 20)

pred = predict(knnFit, newdata = x_validation)
#knnFit$results

xtab = table(pred, y_validation)
cm = confusionMatrix(xtab)
cm = cm$table
round((accuracy <- sum(diag(cm)) / sum(cm)), 3)
```
```{r}
get.conf.stats <- function(cm) {
    out <- vector("list", length(cm))
    for (i in seq_along(cm)) {
        x <- cm[[i]]
        tp <- x$table[x$positive, x$positive] 
        fp <- sum(x$table[x$positive, colnames(x$table) != x$positive])
        fn <- sum(x$table[colnames(x$table) != x$positie, x$positive])
        # TNs are not well-defined for one-vs-all approach
        elem <- c(tp = tp, fp = fp, fn = fn)
        out[[i]] <- elem
    }
    df <- do.call(rbind, out)
    rownames(df) <- unlist(lapply(cm, function(x) x$positive))
    return(as.data.frame(df))
}
get.micro.f1 <- function(cm) {
    cm.summary <- get.conf.stats(cm)
    tp <- sum(cm.summary$tp)
    fn <- sum(cm.summary$fn)
    fp <- sum(cm.summary$fp)
    pr <- tp / (tp + fp)
    re <- tp / (tp + fn)
    f1 <- 2 * ((pr * re) / (pr + re))
    return(f1)
}
df <- data.frame("Prediction" = pred, "Reference" = y_validation, stringsAsFactors=TRUE)
cm <- vector("list", length(levels(df$Reference)))
for (i in seq_along(cm)) {
    positive.class <- levels(df$Reference)[i]
    # in the i-th iteration, use the i-th class as the positive class
    cm[[i]] <- confusionMatrix(df$Prediction, df$Reference, 
                               positive = positive.class)
}
micro.f1 <- get.micro.f1(cm)
get.macro.f1 <- function(cm) {
    c <- cm[[1]]$byClass # a single matrix is sufficient
    re <- sum(c[, "Recall"]) / nrow(c)
    pr <- sum(c[, "Precision"]) / nrow(c)
    f1 <- 2 * ((re * pr) / (re + pr))
    return(f1)
}
macro.f1 <- get.macro.f1(cm)
print(paste0("Micro F1 is: ", round(micro.f1, 3)))
print(paste0("Macro F1 is: ", round(macro.f1, 3)))
```

```{r}
cat("Accuracy: ", Accuracy(pred, y_validation), " Recall: ", Recall(pred, y_validation), " Precision: ", Precision(pred, y_validation), " F1-Score: ", F1_Score(pred, y_validation), "\n")
```
